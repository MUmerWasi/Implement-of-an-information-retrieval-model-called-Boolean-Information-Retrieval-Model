{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverted Index\n",
      "Result-set: [1, 6, 8, 11, 12, 14, 25, 27, 28, 29]\n"
     ]
    }
   ],
   "source": [
    "import pickle, os, string, re, copy\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "#This is the class definition\n",
    "class Posting_List(object):\n",
    "    def __init__(self):\n",
    "#Here we are initializing the class instance variables, total_count is set to 0,\n",
    "#token is an empty string and occurrance is an empty dictionary\n",
    "        self.total_count = 0\n",
    "        self.token = ''\n",
    "        self.occurrance = {\n",
    "        }\n",
    "#This is the representation of the instance when it is printed \n",
    "    def __repr__(self):\n",
    "#Here we are getting the keys of the occurrance dictionary \n",
    "        dict = self.occurrance.keys()\n",
    "#We are casting the keys to a list object so that we can iterate over them\n",
    "        lister = list(dict)\n",
    "#We are checking if the list is not empty, if it is not empty then we are returning the list of doc_ids\n",
    "        if lister:\n",
    "            return f'Result-set: {lister}'\n",
    "#if the list is empty we are returning an empty result set\n",
    "        else:\n",
    "            return 'Result_set:   '\n",
    "#This is to get the length of the occurrance dictionary\n",
    "    def __len__(self):\n",
    "        return len(self.occurrance)\n",
    "#The appendoccurence method adds the doc_id and position to the occurrance dictionary\n",
    "    def appendoccurence(self, doc_id, position):\n",
    "#Here we are incrementing the total count\n",
    "        self.total_count += 1\n",
    "#Here we are checking if the doc_id is not in the occurrance dictionary, if it is not present we are adding it to the dictionary\n",
    "        if doc_id not in self.occurrance.keys():\n",
    "            self.occurrance[doc_id] = []\n",
    "#Here we are appending the position to the doc_id in the occurrance dictionary\n",
    "        self.occurrance[doc_id].append(position)\n",
    "    \n",
    "class InvertedIndex(object):\n",
    "    # Constructor to initialize the index and document dictionaries\n",
    "    def __init__(self):\n",
    "        self.index = {}     # dictionary to store the inverted index\n",
    "        self.docs = {}      # dictionary to store the documents\n",
    "\n",
    "    # Method to get the posting list of a given term\n",
    "    def get_term_postings(self, term):\n",
    "        if term in self.index.keys():   # if the term is present in the index\n",
    "            return self.index[term]    # return its posting list\n",
    "        else:\n",
    "            return f'{term} is not present'+ Posting_List()   # otherwise, return a message indicating that the term is not present\n",
    "    \n",
    "    # Method to get the length of the index\n",
    "    def __len__(self):\n",
    "        return len(self.index.keys())\n",
    "\n",
    "\n",
    "# Remove Punctuation\n",
    "def remove_punctuation(word):\n",
    "    return word.translate(word.maketrans('','',string.punctuation))\n",
    "\n",
    "vocab = set()\n",
    "doc_contents = []\n",
    "Invert_Index_Main = InvertedIndex()\n",
    "printable = set(string.printable) \n",
    "# Printable characters are\n",
    "# 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\n",
    "# !\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~ \\t\\n\\r\\x0b\\x0c\n",
    "\n",
    "# Initialize the stemmer and stop words set\n",
    "query_parser = PorterStemmer()\n",
    "stop_words = set()\n",
    "# Load stop words from file\n",
    "with open('Stopword-List.txt', 'r') as stopwordlist:\n",
    "    lines = stopwordlist.readlines()\n",
    "    for line in lines:\n",
    "        stop_word = line.strip()\n",
    "        if stop_word:\n",
    "            stop_words.add(stop_word)\n",
    "\n",
    "# Loop through all files\n",
    "for file_number in range(1, 31):\n",
    "    # Open the file and read lines\n",
    "    with open(f'{file_number}.txt', 'r') as file1:\n",
    "        lines = file1.readlines()\n",
    "        # Initialize position and document set for this file\n",
    "        position = {'doc': file_number, 'row': 0, 'col': 0, 'token_no': 0}\n",
    "        ddocuments = set()\n",
    "        \n",
    "        # Loop through each line in the file\n",
    "        for line_no, line in enumerate(lines):\n",
    "            # Update position\n",
    "            position['row'] = line_no \n",
    "            position['col'] = 0\n",
    "            \n",
    "            # Split the line into words\n",
    "            words = re.split('[.\\s,?!:;-]', line)\n",
    "            \n",
    "            # Loop through each word in the line\n",
    "            for word in words:\n",
    "                # Update position and token number\n",
    "                position['col'] += len(word) + 1\n",
    "                position['token_no'] += 1\n",
    "                \n",
    "                # Preprocess the word\n",
    "                word = word.lower() # Case folding\n",
    "                word = ''.join(filter(lambda x: x in printable, word)) # Filter non-ASCII characters\n",
    "                word = remove_punctuation(word) # Remove punctuations\n",
    "                \n",
    "                if re.match('\\d+[A-Za-z]+', word):\n",
    "                    word = re.split('\\d+', word)[1]\n",
    "                if re.match('[A-Za-z]+\\d+', word):\n",
    "                    word = re.split('\\d+', word)[0]\n",
    "                \n",
    "                # Skip the word if it is a stop word or empty\n",
    "                if len(word) <= 1 or word in stop_words:\n",
    "                    continue\n",
    "                \n",
    "                # Stem the word\n",
    "                word = query_parser.stem(word)\n",
    "                \n",
    "                # Add the word to vocabulary and document set\n",
    "                vocab.add(word)               \n",
    "                ddocuments.add(word)\n",
    "                \n",
    "                # Update the inverted index\n",
    "                if word in Invert_Index_Main.index:\n",
    "                    Invert_Index_Main.index[word].appendoccurence(file_number, copy.deepcopy(position)) \n",
    "                else:\n",
    "                    plist = Posting_List()\n",
    "                    Invert_Index_Main.index[word] = plist\n",
    "                    Invert_Index_Main.index[word].appendoccurence(file_number, copy.deepcopy(position))\n",
    "                    \n",
    "        # Add the document set to the inverted index\n",
    "        Invert_Index_Main.docs[file_number] = ddocuments\n",
    "        doc_contents.append(ddocuments)\n",
    "# Clean Query Term\n",
    "def improvise_text(word):\n",
    "    # Case Folding\n",
    "    word = word.lower()\n",
    "    # Filter non-alphanumeric characters\n",
    "    word = ''.join(filter(lambda x: x.isalnum(), word))\n",
    "    # Remove Punctuations, digits and split numbers from alpha-numeric strings\n",
    "    if not (word == '(' or word == ')'):\n",
    "        word = re.split('\\d+', word)[0] or re.split('\\D+', word)[-1]\n",
    "    # Skip stemming if already in stemmed form\n",
    "    if not word.endswith('ly') and not word.endswith('ed') and not word.endswith('ing'):\n",
    "        word = query_parser.stem(word)\n",
    "    return word\n",
    "\n",
    "# To Save Inverted Index to File\n",
    "with open('Invert_Index_Main.p', 'wb') as index_file:\n",
    "    pickle.dump(Invert_Index_Main, index_file)\n",
    "\n",
    "# To Load Inverted Index from File\n",
    "try:\n",
    "    with open('Invert_Index_Main.p', 'rb') as index_file:\n",
    "        Invert_Index_Main = pickle.load(index_file)\n",
    "except FileNotFoundError:\n",
    "    print(\"Inverted Index file not found!\")\n",
    "\n",
    "# Function to compute the intersection of two posting lists\n",
    "def intersection(p1, p2):\n",
    "    if len(p1) == 0 or len(p2) == 0:   # if one of the posting lists is empty\n",
    "        return Posting_List()         # return an empty posting list\n",
    "\n",
    "    if isinstance(p1, set) and isinstance(p2, set):   # if both posting lists are sets\n",
    "        return p1.intersection(p2)    # compute the intersection using the set intersection method\n",
    "    elif isinstance(p1, set):         # if only p1 is a set\n",
    "        return p1.intersection(p2.occurrance.keys())  # compute the intersection using p1 as a set and p2's keys\n",
    "    elif isinstance(p2, set):         # if only p2 is a set\n",
    "        return p2.intersection(p1.occurrance.keys())  # compute the intersection using p2 as a set and p1's keys\n",
    "\n",
    "    # If both posting lists are not sets, create a new posting list\n",
    "    pn = Posting_List()\n",
    "    for pn_keys in (p1.occurrance.keys() & p2.occurrance.keys()):  # iterate over the common keys of p1 and p2\n",
    "        pn.appendoccurence(pn_keys, p1.occurrance[pn_keys])  # add the occurrences of the key in p1\n",
    "        pn.appendoccurence(pn_keys, p2.occurrance[pn_keys])  # add the occurrences of the key in p2\n",
    "    return pn\n",
    "\n",
    "\n",
    "def union(p1, p2):\n",
    "    if len(p1) == 0:\n",
    "        return p2\n",
    "    elif len(p2) == 0:\n",
    "        return p1\n",
    "    if isinstance(p1, set) and isinstance(p2, set):\n",
    "        return p1.union(p2)\n",
    "    elif isinstance(p1, set):\n",
    "        return p1.union(p2.occurrance.keys())\n",
    "    elif isinstance(p2, set):\n",
    "        return p2.union(p1.occurance.keys())\n",
    "    \n",
    "    pn = Posting_List()\n",
    "    for pn1_keys in p1.occurrance.keys() :\n",
    "        pn.appendoccurence(pn1_keys, p1.occurrance[pn1_keys])\n",
    "    for pn2_keys in p2.occurrance.keys() :\n",
    "        pn.appendoccurence(pn2_keys, p2.occurrance[pn2_keys])\n",
    "    \n",
    "    return pn\n",
    "\n",
    "# Function to find the documents that do not contain a given query term\n",
    "def inverted_post(Invert_Index_Main, p):\n",
    "    print(p)  # print the query term or set of terms\n",
    "    if isinstance(p, set):  # if p is a set of terms\n",
    "        print('Returning ')\n",
    "        print(set(Invert_Index_Main.docs).difference(p))\n",
    "        return set(Invert_Index_Main.docs).difference(p)  # return the set of all documents minus the query terms\n",
    "    else:  # if p is a single query term\n",
    "        print(set(Invert_Index_Main.docs).difference(set(p.occurrance.keys())))\n",
    "        return set(Invert_Index_Main.docs).difference(set(p.occurrance.keys()))  # return the set of all documents minus the documents containing the query term\n",
    "def construct_indices():\n",
    "    # Set path to data directory\n",
    "    path_to_data = os.path.dirname(__file__) + '../../data/'\n",
    "\n",
    "    # Initialize empty vocabulary set and document contents list\n",
    "    vocab = set()\n",
    "    doc_contents = []\n",
    "\n",
    "    # Create a new instance of the InvertedIndex class\n",
    "    Invert_Index_Main = InvertedIndex()\n",
    "\n",
    "    # Set of printable characters\n",
    "    printable = set(string.printable)\n",
    "\n",
    "    # Initialize a PorterStemmer object for stemming words\n",
    "    query_parser = PorterStemmer()\n",
    "\n",
    "    # Initialize a set of stop words\n",
    "    stop_words = set()\n",
    "\n",
    "    # Load stop words from file\n",
    "    with open(path_to_data+'Stopword-List.txt', 'r') as stopwordlist:\n",
    "        lines = stopwordlist.readlines()\n",
    "        for line in lines:\n",
    "            stop_words.add(line.split('\\n')[0])\n",
    "        stop_words.remove('')\n",
    "\n",
    "    # Loop over all files in the data directory\n",
    "    for file_number in range(1, 31):\n",
    "        # Open file\n",
    "        with open(path_to_data + f'{file_number}.txt', 'r') as file1:\n",
    "            # Read lines from file\n",
    "            lines = file1.readlines()\n",
    "\n",
    "            # Print file number and first line of file\n",
    "            print(f'File Number : {file_number}.txt' )\n",
    "            print(lines[0])\n",
    "\n",
    "            # Initialize position dictionary for keeping track of token positions\n",
    "            position = {'doc':file_number,'row':0, 'col':0, 'token_no':0}\n",
    "\n",
    "            # Loop over lines in file\n",
    "            for line_no,line in enumerate(lines):\n",
    "                # Initialize empty set for current document's words\n",
    "                ddocuments = set()\n",
    "\n",
    "                # Split line into words at . , whitespace ? ! : ;\n",
    "                position['row'] = line_no \n",
    "                position['col'] = 0\n",
    "                for word in re.split('[.\\s,?!:;-]', line):\n",
    "                    # Update position\n",
    "                    position['col'] += len(word) + 1\n",
    "                    position['token_no'] += 1\n",
    "\n",
    "                    # Case folding\n",
    "                    word = word.lower()\n",
    "                    \n",
    "                    # Filter non-ASCII characters\n",
    "                    word = ''.join(filter(lambda x: x in printable, word))\n",
    "                    \n",
    "                    # Remove punctuations\n",
    "                    word = remove_punctuation(word)\n",
    "\n",
    "                    # Remove digits attached to alphabets\n",
    "                    if re.match('\\d+[A-Za-z]+',word):\n",
    "                        word = re.split('\\d+',word)[1]\n",
    "                    # Remove alphabets attached to digits\n",
    "                    if re.match('[A-Za-z]+\\d+',word):\n",
    "                        word = re.split('\\d+',word)[0]\n",
    "                    \n",
    "                    # Skip word if it is a stop word, empty or has only one character\n",
    "                    if len(word) == 0 or len(word) == 1 or word == '' or word == ' ':\n",
    "                        continue\n",
    "                    if word in stop_words:\n",
    "                        continue\n",
    "\n",
    "                    # Stem word using PorterStemmer\n",
    "                    word = query_parser.stem(word)                        \n",
    "                    vocab.add(word)                    \n",
    "                    ddocuments.add(word)\n",
    "\n",
    "                    # Add word to inverted index\n",
    "                    if word in Invert_Index_Main.index.keys():\n",
    "                        Invert_Index_Main.index[word].appendoccurence(file_number, copy.deepcopy(position)) \n",
    "                    else:\n",
    "                        plist = Posting_List()\n",
    "                        Invert_Index_Main.index[word] = plist\n",
    "                        Invert_Index_Main.index[word].appendoccurence(file_number, copy.deepcopy(position))\n",
    "                        \n",
    "            Invert_Index_Main.docs[file_number] = ddocuments\n",
    "            doc_contents.append(ddocuments)\n",
    "    ii = InvertedIndex()\n",
    "    ii.status = True\n",
    "    ii.data = Invert_Index_Main\n",
    "    ii.save()\n",
    "    return True\n",
    "\n",
    "\n",
    "binary = ['and', 'or']\n",
    "unary = ['not']\n",
    "\n",
    "# Token types\n",
    "LPAREN, RPAREN, EOF, TERM, AND, OR, NOT = (\n",
    "    '(', ')', 'EOF', 'TERM', 'AND','OR', 'NOT'\n",
    ")\n",
    "\n",
    "\n",
    "class Token(object):\n",
    "    def __init__(self, type, value):\n",
    "        self.type = type\n",
    "        self.value = value\n",
    "        self.inverse = False\n",
    "        self.row = []\n",
    "\n",
    "    def __str__(self):\n",
    "        #String representation of the class instance.\n",
    "\n",
    "        return 'Token({type}, {value})'.format(\n",
    "            type=self.type,\n",
    "            value=repr(self.value)\n",
    "        )\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "\n",
    "class Lexer(object):\n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "        self.pos = 0\n",
    "        self.current_char = self.text[self.pos]\n",
    "        \n",
    "\n",
    "    def error(self):\n",
    "        raise Exception('Invalid character')\n",
    "\n",
    "    def advance(self):\n",
    "        self.pos += 1\n",
    "        if self.pos > len(self.text) - 1:\n",
    "            self.current_char = None  # Indicates end of input\n",
    "        else:\n",
    "            self.current_char = self.text[self.pos]\n",
    "\n",
    "    def skip_whitespace(self):\n",
    "        while self.current_char is not None and self.current_char.isspace():\n",
    "            self.advance()\n",
    "\n",
    "    def integer(self):\n",
    "        result = ''\n",
    "        while self.current_char is not None and self.current_char.isdigit():\n",
    "            result += self.current_char\n",
    "            self.advance()\n",
    "        return int(result)\n",
    "    \n",
    "    def word(self):\n",
    "        result = ''\n",
    "        # while self.current_char is not None and (self.current_char.isalpha() or self.current_char == '_'):\n",
    "        while self.current_char is not None and (self.current_char in printable) and (self.current_char not in (' ', '|','&','!', '(', ')')):\n",
    "            result += self.current_char\n",
    "            self.advance()\n",
    "        return str(result)\n",
    "\n",
    "    def get_next_token(self):\n",
    "    # Mapping between characters and tokens\n",
    "        char_to_token = {\n",
    "            '&': Token(AND, 'AND'),\n",
    "            '|': Token(OR, 'OR'),\n",
    "            '!': Token(NOT, 'NOT'),\n",
    "            '(': Token(LPAREN, '('),\n",
    "            ')': Token(RPAREN, ')')\n",
    "        }\n",
    "\n",
    "    # Buffer for storing a word\n",
    "        wbuff = ''\n",
    "\n",
    "        while self.current_char is not None:\n",
    "            if self.current_char.isspace():\n",
    "                self.skip_whitespace()\n",
    "                continue\n",
    "\n",
    "            if self.current_char in char_to_token:\n",
    "                token = char_to_token[self.current_char]\n",
    "                self.advance()\n",
    "                return token\n",
    "\n",
    "            if self.current_char.isalpha():\n",
    "            # Use a buffer to store the characters of a word\n",
    "                wbuff = self.current_char\n",
    "                self.advance()\n",
    "                while self.current_char is not None and self.current_char.isalpha():\n",
    "                    wbuff += self.current_char\n",
    "                    self.advance()\n",
    "                return Token(TERM, wbuff)\n",
    "\n",
    "            self.error()\n",
    "\n",
    "        return Token(EOF, None)\n",
    "\n",
    "class AST(object):\n",
    "    pass\n",
    "\n",
    "\n",
    "class Operat(AST):\n",
    "    def __init__(self, left, op, right):\n",
    "        self.left = left\n",
    "        self.token = self.op = op\n",
    "        self.right = right\n",
    "        self.inverse = False\n",
    "        self.row = []\n",
    "        self.value = ''\n",
    "        \n",
    "class Num(AST):\n",
    "    def __init__(self, token):\n",
    "        self.token = token\n",
    "        self.value = token.value\n",
    "        self.inverse = False\n",
    "        self.row = []\n",
    "\n",
    "class Parser(object):\n",
    "    def __init__(self, lexer):\n",
    "        self.lexer = lexer\n",
    "        # set current token to the first token taken from the input\n",
    "        self.current_token = self.lexer.get_next_token()\n",
    "\n",
    "    def error(self):\n",
    "        raise Exception('Invalid syntax')\n",
    "\n",
    "    def eat(self, token_type):\n",
    "        # compare the current token type with the passed token\n",
    "        # type and if they match then \"eat\" the current token\n",
    "        # and assign the next token to the self.current_token,\n",
    "        # otherwise raise an exception.\n",
    "        if self.current_token.type == token_type:\n",
    "            self.current_token = self.lexer.get_next_token()\n",
    "        else:\n",
    "            self.error()\n",
    "\n",
    "    def factor(self):\n",
    "        \"\"\"factor : INTEGER | LPAREN expr RPAREN\"\"\"\n",
    "        token = self.current_token\n",
    "        \n",
    "        \n",
    "        if token.type == TERM:\n",
    "            self.eat(TERM)\n",
    "            return Num(token)\n",
    "        \n",
    "        elif token.type == NOT:\n",
    "            self.eat(NOT)\n",
    "            node = self.expr()\n",
    "            node.inverse = True\n",
    "            return node\n",
    "            \n",
    "        \n",
    "        \n",
    "        elif token.type == LPAREN:\n",
    "            self.eat(LPAREN)\n",
    "            node = self.expr()\n",
    "            self.eat(RPAREN)\n",
    "            return node\n",
    "\n",
    "    def term(self):\n",
    "        node = self.factor()\n",
    "        while self.current_token.type in (AND,):\n",
    "            token = self.current_token\n",
    "         \n",
    "            if token.type == AND:\n",
    "                self.eat(AND)\n",
    "\n",
    "            node = Operat(left=node, op=token, right=self.factor())\n",
    "\n",
    "        return node\n",
    "\n",
    "    def expr(self):\n",
    "        node = self.term()\n",
    "        while self.current_token.type in (OR,):\n",
    "            token = self.current_token\n",
    "            if token.type == OR:\n",
    "                self.eat(OR)\n",
    "            \n",
    "            node = Operat(left=node, op=token, right=self.term())\n",
    "        \n",
    "        return node\n",
    "\n",
    "    def parse(self):\n",
    "        return self.expr()\n",
    "\n",
    "\n",
    "class NodeVisitor(object):\n",
    "    def visit(self, node):\n",
    "        method_name = 'visit_' + type(node).__name__\n",
    "        visitor = getattr(self, method_name, self.generic_visit)\n",
    "        return visitor(node)\n",
    "\n",
    "    def generic_visit(self, node):\n",
    "        raise Exception('No visit_{} method'.format(type(node).__name__))\n",
    "\n",
    "\n",
    "class Interpreter(NodeVisitor):\n",
    "    def __init__(self, parser, index, query_parser):\n",
    "        # Initialize the Interpreter with a Parser, an Index, and a Query Parser\n",
    "        self.parser = parser\n",
    "        self.index = index\n",
    "        self.query_parser = query_parser\n",
    "\n",
    "    def visit_Operat(self, node):\n",
    "        # Visit the left and right children of the current node\n",
    "        left = self.visit(node.left)   \n",
    "        right = self.visit(node.right)\n",
    "    \n",
    "        # If the operator is an AND operator\n",
    "        if node.op.type == AND:\n",
    "            # If the left node is inverted, add a negation symbol to its value and set its inverse flag to False\n",
    "            if left.inverse:\n",
    "                left.value = '!' + str(left.value)\n",
    "                left.inverse = False\n",
    "            # If the right node is inverted, add a negation symbol to its value and set its inverse flag to False\n",
    "            if right.inverse:\n",
    "                right.value = '!' + str(right.value)\n",
    "                right.inverse = False\n",
    "            # Compute the intersection of the posting lists of the left and right nodes\n",
    "            node.row = intersection(left.row, right.row)\n",
    "        # If the operator is an OR operator\n",
    "        elif node.op.type == OR:\n",
    "            # If the left node is inverted, add a negation symbol to its value and set its inverse flag to False\n",
    "            if left.inverse:\n",
    "                left.value = '!' + str(left.value)          \n",
    "                left.inverse = False\n",
    "            # If the right node is inverted, add a negation symbol to its value and set its inverse flag to False\n",
    "            if right.inverse:\n",
    "                right.value = '!' + str(right.value)            \n",
    "                right.inverse = False\n",
    "            # Compute the union of the posting lists of the left and right nodes\n",
    "            node.row = union(left.row, right.row)\n",
    "    \n",
    "        # If the current node is inverted\n",
    "        if node.inverse:\n",
    "            # Compute the complement of its posting list\n",
    "            node.row = inverted_post(self.index, node.row)\n",
    "            # Set its inverse flag to False\n",
    "            node.inverse = False\n",
    "    \n",
    "        # Return the current node\n",
    "        return node\n",
    "       \n",
    "    def visit_Num(self, node):\n",
    "        node.value = node.value.split('_')[0]\n",
    "        if self.query_parser.stem(node.value) in self.index.index.keys():\n",
    "            term_docs = self.index.index[self.query_parser.stem(node.value)]\n",
    "        else:\n",
    "            term_docs = {}\n",
    "\n",
    "        node.row = term_docs\n",
    "        if node.inverse == True:\n",
    "            node.row = inverted_post(self.index, node.row)\n",
    "            node.inverse = False\n",
    "        return node\n",
    "\n",
    "    def interpret(self):\n",
    "        tree = self.parser.parse()\n",
    "        return self.visit(tree)\n",
    "\n",
    "'''\n",
    "This function takes a query in natural language as an input and converts it into a boolean query.\n",
    "It uses the PorterStemmer and Lexer objects from nltk library to tokenize the natural language query and identify the words.\n",
    "The Parser class from nltk is used to parse the tokens using the given grammar.\n",
    "The Interpreter class from nltk is used to interpret the parsed tokens and convert the natural language query into a boolean query.\n",
    "The boolean query is then returned as the output.\n",
    "'''\n",
    "def get_boolean_query(query):\n",
    "    text = str(query)\n",
    "    text = text.replace(' and ','&')\n",
    "    text = text.replace(' AND ','&')\n",
    "    text = text.replace(' or ','|')\n",
    "    text = text.replace(' OR ','|')\n",
    "    text = text.replace('NOT', '!')\n",
    "    text = text.replace('not ','!')\n",
    "\n",
    "    query_parser = PorterStemmer()\n",
    "    lexer = Lexer(text)\n",
    "    parser = Parser(lexer)\n",
    "    interpreter = Interpreter(parser, Invert_Index_Main, query_parser)\n",
    "    result = interpreter.interpret()\n",
    "    return result.row\n",
    "\n",
    "def positional_intersect(p1, p2, k):\n",
    "    \n",
    "    ip = intersection(p1, p2)\n",
    "    lip = sorted(list(ip.occurrance))\n",
    "    npl = Posting_List()\n",
    "    ans = []\n",
    "    \n",
    "    for doc in lip:\n",
    "        positions1 = p1.occurrance[doc]\n",
    "        positions2 = p2.occurrance[doc]\n",
    "        for pos1 in positions1:\n",
    "            for pos2 in positions2:\n",
    "                if pos2['token_no'] -  pos1['token_no'] == k and pos2['token_no'] -  pos1['token_no'] > 0:\n",
    "                    ans.append({'doc':doc, 'pos1':  pos1, 'pos2':pos2})\n",
    "                    npl.appendoccurence(doc,pos1)\n",
    "                    npl.appendoccurence(doc,pos2)\n",
    "    return npl\n",
    "        \n",
    "\n",
    "def get_phrasal_query(query):\n",
    "    text = str(query)\n",
    "    try:\n",
    "        q1, q2 = text.split(' ')\n",
    "    except ValueError as e:\n",
    "        raise ValueError('Invalid Phrasal Query Syntax')\n",
    "    query_parser = PorterStemmer()\n",
    "    q1 = query_parser.stem(q1)\n",
    "    q2 = query_parser.stem(q2)\n",
    "    result = [] \n",
    "    p1 = Invert_Index_Main.get_term_postings(q1)\n",
    "    p2 = Invert_Index_Main.get_term_postings(q2)\n",
    "    \n",
    "    result = positional_intersect(p1, p2, 1)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def get_proximity_query(query):\n",
    "    text = str(query)\n",
    "    try:\n",
    "        q1, q2, q3 = text.split(' ')\n",
    "    except ValueError as e:\n",
    "        raise ValueError('Invalid Proximity Query Syntax')\n",
    "    query_parser = PorterStemmer()\n",
    "    q1 = query_parser.stem(q1)\n",
    "    q2 = query_parser.stem(q2)\n",
    "    k = int(q3[1])+ 1\n",
    "    print('Inverted Index')\n",
    "    result = [] \n",
    "    \n",
    "    p1 = Invert_Index_Main.get_term_postings(q1)\n",
    "    p2 = Invert_Index_Main.get_term_postings(q2)\n",
    "    print(p1)\n",
    "    result = positional_intersect(p1, p2, k-1)\n",
    "\n",
    "    return result\n",
    "\n",
    "#GUI Implementation using Tkinter modules\n",
    "import tkinter as tk\n",
    "\n",
    "def open_boolean_query_window():\n",
    "    boolean_query_window = tk.Toplevel(root)\n",
    "    boolean_query_window.title(\"Boolean Query\")\n",
    "\n",
    "    query_label = tk.Label(boolean_query_window, text=\"Enter Boolean Query: (batter and bowler) or (batter or bowler)\", font='Arial 10 bold')\n",
    "    query_label.pack()\n",
    "\n",
    "    query_entry = tk.Entry(boolean_query_window, width=50)\n",
    "    query_entry.pack()\n",
    "\n",
    "    def get_query():\n",
    "        q = query_entry.get()\n",
    "        result = get_boolean_query(q)\n",
    "        output_box.delete(1.0, tk.END) # clear previous output\n",
    "        output_box.insert(tk.END, result)\n",
    "\n",
    "    submit_button = tk.Button(boolean_query_window, text=\"Submit\", command=get_query, width=20, height=2, font='Arial 10 bold')\n",
    "    submit_button.pack()\n",
    "\n",
    "    output_label = tk.Label(boolean_query_window, text=\"Result:\", font='Arial 10 bold')\n",
    "    output_label.pack()\n",
    "\n",
    "    output_box = tk.Text(boolean_query_window, height=10, width=50)\n",
    "    output_box.pack()\n",
    "\n",
    "def open_phrasal_query_window():\n",
    "    phrasal_query_window = tk.Toplevel(root)\n",
    "    phrasal_query_window.title(\"Phrasal Query\")\n",
    "\n",
    "    query_label = tk.Label(phrasal_query_window, text=\"Enter phrasal Query: [Babar Azam]\", font='Arial 10 bold')\n",
    "    query_label.pack()\n",
    "\n",
    "    query_entry = tk.Entry(phrasal_query_window, width=50)\n",
    "    query_entry.pack()\n",
    "\n",
    "    def get_query():\n",
    "        q = query_entry.get()\n",
    "        result = get_phrasal_query(q)\n",
    "        output_box.delete(1.0, tk.END) # clear previous output\n",
    "        output_box.insert(tk.END, result)\n",
    "\n",
    "    submit_button = tk.Button(phrasal_query_window, text=\"Submit\", command=get_query, width=20, height=2, font='Arial 10 bold')\n",
    "    submit_button.pack()\n",
    "\n",
    "    output_label = tk.Label(phrasal_query_window, text=\"Result:\", font='Arial 10 bold')\n",
    "    output_label.pack()\n",
    "\n",
    "    output_box = tk.Text(phrasal_query_window, height=10, width=50)\n",
    "    output_box.pack()\n",
    "\n",
    "def open_proximity_query_window():\n",
    "    proximity_query_window = tk.Toplevel(root)\n",
    "    proximity_query_window.title(\"Proximity Query\")\n",
    "\n",
    "    query_label = tk.Label(proximity_query_window, text='Enter Proximity Query: [fielder batter /2] : ', font='Arial 10 bold')\n",
    "    query_label.pack()\n",
    "\n",
    "    query_entry = tk.Entry(proximity_query_window, width=50)\n",
    "    query_entry.pack()\n",
    "\n",
    "    def get_query():\n",
    "        q = query_entry.get()\n",
    "        result = get_proximity_query(q)\n",
    "        output_box.delete(1.0, tk.END) # clear previous output\n",
    "        output_box.insert(tk.END, result)\n",
    "\n",
    "    submit_button = tk.Button(proximity_query_window, text=\"Submit\", command=get_query, width=20, height=2, font='Arial 10 bold')\n",
    "    submit_button.pack()\n",
    "\n",
    "    output_label = tk.Label(proximity_query_window, text=\"Result:\", font='Arial 10 bold')\n",
    "    output_label.pack()\n",
    "\n",
    "    output_box = tk.Text(proximity_query_window, height=10, width=50)\n",
    "    output_box.pack()\n",
    "\n",
    "def Info():\n",
    "    import tkinter as tk\n",
    "    root = tk.Tk()\n",
    "    root.title('Inverted Index Stats')\n",
    "    root.geometry('300x100')\n",
    "    vocabulary_size = len(Invert_Index_Main.index.keys())\n",
    "    num_documents = len(Invert_Index_Main.docs)\n",
    "    message = f'Total Vocabulary Size: {vocabulary_size}\\nTotal Number of Documents: {num_documents}'\n",
    "    label = tk.Label(root, text=message)\n",
    "    label.pack(pady=20)\n",
    "    root.after(10000, lambda: root.destroy())\n",
    "\n",
    "\n",
    "def exit_program():\n",
    "    root.destroy()\n",
    "\n",
    "root = tk.Tk()\n",
    "root.title(\"Information Retrieval Assignment 1\")\n",
    "root.geometry(\"500x350\")\n",
    "\n",
    "option1_button = tk.Button(root, text=\"Boolean Query\", command=open_boolean_query_window, width=20, height=2, font='Arial 10 bold')\n",
    "option1_button.pack()\n",
    "\n",
    "option2_button = tk.Button(root, text=\"Phrasal Query\", command=open_phrasal_query_window, width=20, height=2, font='Arial 10 bold')\n",
    "option2_button.pack()\n",
    "\n",
    "option3_button = tk.Button(root, text=\"Proximity Query\", command=open_proximity_query_window, width=20, height=2, font='Arial 10 bold')\n",
    "option3_button.pack()\n",
    "\n",
    "option4_button = tk.Button(root, text=\"Dataset Information\", command=Info, width=20, height=2, font='Arial 10 bold')\n",
    "option4_button.pack()\n",
    "\n",
    "exit_button = tk.Button(root, text=\"Exit\", command=exit_program, width=20, height=2, font='Arial 10 bold')\n",
    "exit_button.pack()\n",
    "\n",
    "root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
